"""
RAG (Retrieval-Augmented Generation) ì—”ì§„ â€” LangChain ê¸°ë°˜

ì§ˆë¬¸ ë¼ìš°íŒ… â†’ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ â†’ LLM ë‹µë³€ ìƒì„± íŒŒì´í”„ë¼ì¸ì„ í†µí•© ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import json
import shutil
import time
import logging
from datetime import datetime
from pathlib import Path

from dotenv import load_dotenv
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyMuPDFLoader, Docx2txtLoader
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from core.models import get_llm, get_embeddings, DEFAULT_MODEL
from core.router import classify, get_meta_response
from core.memory import rewrite_query, format_history

load_dotenv()

logger = logging.getLogger(__name__)

# â”€â”€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).parent.parent
DATA_DIR = BASE_DIR / "data"
INDEX_DIR = BASE_DIR / "index"
LOG_DIR = BASE_DIR / "logs"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 100
TOP_K = 10

# â”€â”€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ë²”ìš© ì–´ì‹œìŠ¤í„´íŠ¸) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT_RAG = (
    "ë‹¹ì‹ ì€ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
    "ì•„ë˜ ì°¸ê³  ë¬¸ì„œê°€ ì œê³µëœ ê²½ìš°, ë¬¸ì„œ ë‚´ìš©ì„ ê·¼ê±°ë¡œ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
    "ë¬¸ì„œì— ê·¼ê±°í•œ ë‹µë³€ì˜ ê²½ìš° ì¶œì²˜(ë¬¸ì„œëª…)ë¥¼ í•¨ê»˜ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.\n"
    "ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì„ ì§ˆë¬¸ë°›ìœ¼ë©´ 'ì œê³µëœ ë¬¸ì„œì—ì„œ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ë‹µí•˜ì„¸ìš”."
)

SYSTEM_PROMPT_GENERAL = (
    "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
    "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."
)

PROMPT_TEMPLATE_RAG = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_RAG),
    ("human", "{history_block}## ì°¸ê³  ë¬¸ì„œ\n\n{context}\n\n## ì§ˆë¬¸\n\n{question}"),
])

PROMPT_TEMPLATE_GENERAL = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT_GENERAL),
    ("human", "{history_block}{question}"),
])


# â”€â”€ JSONL íŠ¸ë ˆì´ìŠ¤ ë¡œê±° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _save_trace_to_jsonl(trace: dict):
    """trace dictë¥¼ logs/traces.jsonlì— í•œ ì¤„ì”© ì¶”ê°€ ì €ì¥í•©ë‹ˆë‹¤."""
    LOG_DIR.mkdir(exist_ok=True)
    filepath = LOG_DIR / "traces.jsonl"

    record = {
        "timestamp": datetime.now().isoformat(),
        "question": trace.get("question", ""),
        "rewritten_query": trace.get("rewritten_query", ""),
        "route": trace.get("route", ""),
        "answer": trace.get("answer", ""),
        "source": trace.get("source", "unknown"),
        "chat_history_turns": len(trace.get("chat_history", [])),
        "retrieved_chunks": [
            {
                "source": c["source"],
                "page": c["page"],
                "score": c["score"],
                "text_preview": c["text"][:200],
            }
            for c in trace.get("retrieved_chunks", [])
        ],
        "timing": trace.get("timing", {}),
        "token_usage": trace.get("token_usage", {}),
        "model": trace.get("model", ""),
        "embedding_model": trace.get("embedding_model", ""),
    }

    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(record, ensure_ascii=False) + "\n")


class RAG:
    """PDF ê¸°ë°˜ RAG ì‹œìŠ¤í…œ (ë¼ìš°íŒ… + í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰)"""

    def __init__(self, model_name: str | None = None):
        self.llm = get_llm(model_name)
        self.embeddings = get_embeddings()
        self.vectorstore: FAISS | None = None

        # ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸ â†’ ìœ íš¨í•˜ë©´ ë¡œë“œ, ì•„ë‹ˆë©´ ë¹Œë“œ
        if self._cache_is_valid():
            self._load_cache()
        else:
            self._build()

    # â”€â”€ ì¸ë±ìŠ¤ ë¹Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def _build(self):
        """PDF ë¡œë“œ â†’ ì²­í¬ ë¶„í•  â†’ ì„ë² ë”© â†’ FAISS ì¸ë±ìŠ¤ ìƒì„±"""
        print("ğŸ”¨ ì¸ë±ìŠ¤ë¥¼ ìƒˆë¡œ ë¹Œë“œí•©ë‹ˆë‹¤...")

        documents = []
        
        # 1. PDF íŒŒì¼ ë¡œë“œ
        for pdf_path in sorted(DATA_DIR.glob("*.pdf")):
            loader = PyMuPDFLoader(str(pdf_path))
            docs = loader.load()
            documents.extend(docs)
            total_chars = sum(len(d.page_content) for d in docs)
            print(f"  ğŸ“„ [PDF] ë¡œë“œ ì™„ë£Œ: {pdf_path.name} ({total_chars:,}ì, {len(docs)}í˜ì´ì§€)")

        # 2. Word (.docx) íŒŒì¼ ë¡œë“œ
        for docx_path in sorted(DATA_DIR.glob("*.docx")):
            loader = Docx2txtLoader(str(docx_path))
            docs = loader.load()
            documents.extend(docs)
            total_chars = sum(len(d.page_content) for d in docs)
            print(f"  ğŸ“ [Word] ë¡œë“œ ì™„ë£Œ: {docx_path.name} ({total_chars:,}ì)")

        if not documents:
            raise FileNotFoundError(f"data/ í´ë”ì— PDF ë˜ëŠ” Word íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}")

        splitter = RecursiveCharacterTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        chunks = splitter.split_documents(documents)
        print(f"  ğŸ”ª ì´ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

        # ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì•ì— ì¶œì²˜ ë¬¸ì„œëª…ì„ ì‚½ì… (ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒ)
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            source_name = Path(source).name if source else "ì•Œ ìˆ˜ ì—†ìŒ"
            chunk.page_content = f"[ì¶œì²˜: {source_name}]\n{chunk.page_content}"

        self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
        print(f"  âœ… FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ (ë²¡í„° {self.vectorstore.index.ntotal}ê°œ)")
        self._save_cache()

    # â”€â”€ ìºì‹œ ê´€ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    MANIFEST_FILE = INDEX_DIR / "manifest.json"

    def _get_current_file_manifest(self) -> dict:
        """data/ í´ë”ì˜ í˜„ì¬ íŒŒì¼ ëª©ë¡ê³¼ í¬ê¸°ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        data_files = sorted(DATA_DIR.glob("*.pdf")) + sorted(DATA_DIR.glob("*.docx"))
        return {
            f.name: {"size": f.stat().st_size, "mtime": f.stat().st_mtime}
            for f in data_files
        }

    def _cache_is_valid(self) -> bool:
        index_faiss = INDEX_DIR / "index.faiss"
        index_pkl = INDEX_DIR / "index.pkl"
        if not index_faiss.exists() or not index_pkl.exists():
            return False

        current_manifest = self._get_current_file_manifest()

        # ë¬¸ì„œê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ìºì‹œ ë¬´íš¨
        if not current_manifest:
            return False

        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ (êµ¬ë²„ì „ ìºì‹œ) ì¬ë¹Œë“œ
        if not self.MANIFEST_FILE.exists():
            print("ğŸ“¢ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
            return False

        # ì €ì¥ëœ ë§¤ë‹ˆí˜ìŠ¤íŠ¸ì™€ í˜„ì¬ íŒŒì¼ ëª©ë¡ ë¹„êµ
        with open(self.MANIFEST_FILE, "r", encoding="utf-8") as f:
            saved_manifest = json.load(f)

        saved_names = set(saved_manifest.keys())
        current_names = set(current_manifest.keys())

        # íŒŒì¼ ì¶”ê°€ ê°ì§€
        added = current_names - saved_names
        if added:
            print(f"ğŸ“¢ ìƒˆ ë¬¸ì„œ ì¶”ê°€ë¨: {', '.join(added)} â†’ ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
            return False

        # íŒŒì¼ ì‚­ì œ ê°ì§€
        removed = saved_names - current_names
        if removed:
            print(f"ğŸ“¢ ë¬¸ì„œ ì‚­ì œë¨: {', '.join(removed)} â†’ ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
            return False

        # íŒŒì¼ ìˆ˜ì • ê°ì§€ (í¬ê¸° ë˜ëŠ” ìˆ˜ì •ì‹œê°„ ë³€ê²½)
        for name in current_names:
            if current_manifest[name]["size"] != saved_manifest[name]["size"]:
                print(f"ğŸ“¢ ë¬¸ì„œ ë³€ê²½ë¨: {name} â†’ ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
                return False
            if current_manifest[name]["mtime"] > saved_manifest[name]["mtime"]:
                print(f"ğŸ“¢ ë¬¸ì„œ ìˆ˜ì •ë¨: {name} â†’ ì¸ë±ìŠ¤ë¥¼ ì¬ë¹Œë“œí•©ë‹ˆë‹¤.")
                return False

        return True

    def _save_cache(self):
        INDEX_DIR.mkdir(exist_ok=True)
        self.vectorstore.save_local(str(INDEX_DIR))

        # ë§¤ë‹ˆí˜ìŠ¤íŠ¸ ì €ì¥ (í˜„ì¬ íŒŒì¼ ëª©ë¡ ê¸°ë¡)
        manifest = self._get_current_file_manifest()
        with open(self.MANIFEST_FILE, "w", encoding="utf-8") as f:
            json.dump(manifest, f, ensure_ascii=False, indent=2)

        print(f"  ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {INDEX_DIR} (ë¬¸ì„œ {len(manifest)}ê°œ ê¸°ë¡)")

    def _load_cache(self):
        print("ğŸ“‚ ìºì‹œëœ ì¸ë±ìŠ¤ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        self.vectorstore = FAISS.load_local(
            str(INDEX_DIR), self.embeddings, allow_dangerous_deserialization=True
        )
        print(f"  âœ… ë¡œë“œ ì™„ë£Œ (ë²¡í„° {self.vectorstore.index.ntotal}ê°œ)")

    def rebuild(self):
        if INDEX_DIR.exists():
            shutil.rmtree(INDEX_DIR)
        self._build()

    # â”€â”€ ê²€ìƒ‰ (ë””ë²„ê¹…ìš©) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def search(self, question: str, top_k: int = TOP_K) -> list[tuple]:
        return self.vectorstore.similarity_search_with_score(question, k=top_k)

    # â”€â”€ ëª¨ë¸ êµì²´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def set_model(self, model_name: str):
        """ëŸ°íƒ€ì„ì— LLM ëª¨ë¸ì„ êµì²´í•©ë‹ˆë‹¤."""
        self.llm = get_llm(model_name)
        logger.info(f"[ëª¨ë¸ êµì²´] â†’ {model_name}")

    # â”€â”€ í•µì‹¬: ë¼ìš°íŒ… + ë‹µë³€ ìƒì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def ask_with_trace(self, question: str, source: str = "unknown", chat_history: list[dict] | None = None) -> dict:
        """
        ì§ˆë¬¸ì„ ë¼ìš°íŒ… â†’ ê²½ë¡œë³„ ì²˜ë¦¬ â†’ trace ë°˜í™˜

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            source: ìš”ì²­ ì¶œì²˜ ("slack", "dm", "test")
            chat_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"role": "user"|"assistant", "content": "..."}]
        """
        chat_history = chat_history or []

        trace = {
            "question": question,
            "rewritten_query": "",
            "source": source,
            "route": "",
            "chat_history": chat_history,
            "retrieved_chunks": [],
            "context": "",
            "prompt": "",
            "answer": "",
            "timing": {},
            "model": getattr(self.llm, "model_name", str(self.llm)),
            "embedding_model": getattr(self.embeddings, "model", ""),
        }

        if not question.strip():
            trace["answer"] = "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."
            return trace

        t_start = time.time()

        # â”€â”€ STEP 0-1: Query Rewriting (ëŒ€í™” ë§¥ë½ ë°˜ì˜) â”€â”€
        search_query = question  # ë²¡í„° ê²€ìƒ‰ì— ì‚¬ìš©í•  ì§ˆë¬¸
        if chat_history:
            t_rw0 = time.time()
            search_query = rewrite_query(question, chat_history, self.llm)
            t_rw1 = time.time()
            trace["rewritten_query"] = search_query
            trace["timing"]["0_rewriting"] = round(t_rw1 - t_rw0, 3)

        # â”€â”€ STEP 0-2: ë¼ìš°íŒ… (ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ë¶„ë¥˜) â”€â”€
        t0 = time.time()
        route = classify(search_query, self.llm)
        t1 = time.time()
        trace["route"] = route
        trace["timing"]["0_routing"] = round(t1 - t0, 3)

        # â”€â”€ íˆìŠ¤í† ë¦¬ ë¸”ë¡ (í”„ë¡¬í”„íŠ¸ ì‚½ì…ìš©) â”€â”€
        history_block = ""
        if chat_history:
            history_block = f"## ì´ì „ ëŒ€í™”\n\n{format_history(chat_history)}\n\n"

        # â”€â”€ ê²½ë¡œë³„ ì²˜ë¦¬ â”€â”€
        if route == "meta":
            trace["answer"] = get_meta_response(search_query, self.vectorstore)
            trace["timing"]["total"] = round(time.time() - t_start, 3)
            _save_trace_to_jsonl(trace)
            return trace

        if route == "general":
            t2 = time.time()
            prompt_messages = PROMPT_TEMPLATE_GENERAL.format_messages(
                question=question, history_block=history_block
            )
            response = self.llm.invoke(prompt_messages)
            t3 = time.time()
            trace["answer"] = response.content
            trace["timing"]["2_llm_generation"] = round(t3 - t2, 3)
            trace["timing"]["total"] = round(t3 - t_start, 3)
            trace["prompt"] = "\n".join([f"[{m.type}]\n{m.content}" for m in prompt_messages])
            if hasattr(response, "response_metadata"):
                usage = response.response_metadata.get("token_usage", {})
                if usage:
                    trace["token_usage"] = {
                        "prompt_tokens": usage.get("prompt_tokens", 0),
                        "completion_tokens": usage.get("completion_tokens", 0),
                        "total_tokens": usage.get("total_tokens", 0),
                    }
            logger.info(f"[GENERAL] Q: {question[:50]}... | LLM: {trace['timing']['2_llm_generation']}s")
            _save_trace_to_jsonl(trace)
            return trace

        # â”€â”€ route == "document": RAG íŒŒì´í”„ë¼ì¸ â”€â”€
        # STEP 1: ë²¡í„° ê²€ìƒ‰ (ì¬ì‘ì„±ëœ ì§ˆë¬¸ìœ¼ë¡œ ê²€ìƒ‰)
        t2 = time.time()
        results = self.vectorstore.similarity_search_with_score(search_query, k=TOP_K)
        t3 = time.time()
        trace["timing"]["1_retrieval"] = round(t3 - t2, 3)

        for doc, score in results:
            source_file = doc.metadata.get("source", "ì•Œ ìˆ˜ ì—†ìŒ")
            source_name = Path(source_file).name if source_file else "ì•Œ ìˆ˜ ì—†ìŒ"
            trace["retrieved_chunks"].append({
                "source": source_name,
                "page": doc.metadata.get("page", "?"),
                "score": round(float(score), 4),
                "text": doc.page_content,
            })

        # STEP 2: ì»¨í…ìŠ¤íŠ¸ ì¡°í•©
        context_parts = []
        for i, chunk in enumerate(trace["retrieved_chunks"], 1):
            context_parts.append(
                f"[ë¬¸ì„œ {i}] (ì¶œì²˜: {chunk['source']}, p.{chunk['page']})\n{chunk['text']}"
            )
        context = "\n\n---\n\n".join(context_parts)
        trace["context"] = context

        # STEP 3: í”„ë¡¬í”„íŠ¸ ìƒì„± (íˆìŠ¤í† ë¦¬ í¬í•¨)
        prompt_messages = PROMPT_TEMPLATE_RAG.format_messages(
            context=context, question=question, history_block=history_block
        )
        trace["prompt"] = "\n".join([f"[{m.type}]\n{m.content}" for m in prompt_messages])

        # STEP 4: LLM í˜¸ì¶œ
        t4 = time.time()
        response = self.llm.invoke(prompt_messages)
        t5 = time.time()
        trace["timing"]["2_llm_generation"] = round(t5 - t4, 3)
        trace["timing"]["total"] = round(t5 - t_start, 3)
        trace["answer"] = response.content

        if hasattr(response, "response_metadata"):
            usage = response.response_metadata.get("token_usage", {})
            if usage:
                trace["token_usage"] = {
                    "prompt_tokens": usage.get("prompt_tokens", 0),
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "total_tokens": usage.get("total_tokens", 0),
                }

        logger.info(
            f"[RAG] Q: {question[:50]}... | "
            f"ê²€ìƒ‰: {trace['timing'].get('1_retrieval', '?')}s | "
            f"LLM: {trace['timing']['2_llm_generation']}s | "
            f"ì´: {trace['timing']['total']}s"
        )
        _save_trace_to_jsonl(trace)
        return trace

    # â”€â”€ ê°„ë‹¨ ë‹µë³€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def ask(self, question: str, source: str = "unknown") -> str:
        trace = self.ask_with_trace(question, source=source)
        return trace["answer"]


# â”€â”€ ë‹¨ë… ì‹¤í–‰ ì‹œ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    rag = RAG()
    print("\n" + "=" * 60)
    test_q = "ì½”ì¹­ìŠ¤í„°ë”” 17ê¸° ìˆ˜ë£Œìœ¨ì€?"
    print(f"Q: {test_q}")
    print(f"A: {rag.ask(test_q, source='cli')}")
